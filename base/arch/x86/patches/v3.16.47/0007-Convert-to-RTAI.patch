From bd0d854a3789d076efd30c7968f5a25ade69b94d Mon Sep 17 00:00:00 2001
From: Alec Ari <alec@onelabs.com>
Date: Mon, 11 Sep 2017 17:13:20 -0500
Subject: [PATCH 7/7] Convert to RTAI

Includes some indentation fixes and silences
some compiler warnings.

Signed-off-by: Alec Ari <alec@onelabs.com>
---
 arch/x86/kernel/ipipe.c |   9 ++
 init/Kconfig            |   2 +-
 kernel/ipipe/Kconfig    |   1 +
 kernel/ipipe/core.c     | 212 ++++++++++++++++++++----------------------------
 kernel/ipipe/timer.c    |  22 ++---
 kernel/sched/core.c     |  83 +++++++++++--------
 6 files changed, 156 insertions(+), 173 deletions(-)

diff --git a/arch/x86/kernel/ipipe.c b/arch/x86/kernel/ipipe.c
index 542cf16..ba1d5a0 100644
--- a/arch/x86/kernel/ipipe.c
+++ b/arch/x86/kernel/ipipe.c
@@ -58,6 +58,15 @@
 #include <asm/fpu-internal.h>
 #include <asm/mce.h>
 
+EXPORT_SYMBOL(io_apic_irqs);
+EXPORT_SYMBOL(find_task_by_pid_ns);
+extern void *sys_call_table;
+EXPORT_SYMBOL(sys_call_table);
+#ifdef CONFIG_X86_32
+extern spinlock_t i8259A_lock;
+EXPORT_SYMBOL(i8259A_lock);
+#endif
+
 DEFINE_PER_CPU(unsigned long, __ipipe_cr2);
 EXPORT_PER_CPU_SYMBOL_GPL(__ipipe_cr2);
 
diff --git a/init/Kconfig b/init/Kconfig
index 532d35c..534057a 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -69,7 +69,7 @@ config COMPILE_TEST
 
 config LOCALVERSION
 	string "Local version - append to kernel release"
-	default "-ipipe"
+	default "-RTAI"
 	help
 	  Append an extra string to the end of your kernel version.
 	  This will show up when you type uname, for example.
diff --git a/kernel/ipipe/Kconfig b/kernel/ipipe/Kconfig
index da17b04..ca4df2e 100644
--- a/kernel/ipipe/Kconfig
+++ b/kernel/ipipe/Kconfig
@@ -8,6 +8,7 @@ config IPIPE
 config IPIPE_LEGACY
 	bool "I-pipe legacy interface"
 	depends on IPIPE
+	default y
 	---help---
 	  Activate this option if you want to control the interrupt
 	  pipeline via the legacy interface.
diff --git a/kernel/ipipe/core.c b/kernel/ipipe/core.c
index 753be96..35b27f5 100644
--- a/kernel/ipipe/core.c
+++ b/kernel/ipipe/core.c
@@ -985,53 +985,55 @@ void __ipipe_root_sync(void)
 
 int __ipipe_notify_syscall(struct pt_regs *regs)
 {
-	struct ipipe_domain *caller_domain, *this_domain, *ipd;
-	struct ipipe_percpu_domain_data *p;
-	unsigned long flags;
-	int ret = 0;
+	{ return __ipipe_notify_kevent(IPIPE_EVENT_SYSCALL - IPIPE_FIRST_EVENT, regs); }{
+		struct ipipe_domain *caller_domain, *this_domain, *ipd;
+		struct ipipe_percpu_domain_data *p;
+		unsigned long flags;
+		int ret = 0;
 
-	/*
-	 * We should definitely not pipeline a syscall with IRQs off.
-	 */
-	IPIPE_WARN_ONCE(hard_irqs_disabled());
+		/*
+		* We should definitely not pipeline a syscall with IRQs off.
+		*/
+		IPIPE_WARN_ONCE(hard_irqs_disabled());
 
-	flags = hard_local_irq_save();
-	caller_domain = this_domain = __ipipe_current_domain;
-	ipd = ipipe_head_domain;
-next:
-	p = ipipe_this_cpu_context(ipd);
-	if (likely(p->coflags & __IPIPE_SYSCALL_E)) {
-		__ipipe_set_current_context(p);
-		p->coflags |= __IPIPE_SYSCALL_R;
-		hard_local_irq_restore(flags);
-		ret = ipipe_syscall_hook(caller_domain, regs);
 		flags = hard_local_irq_save();
-		p->coflags &= ~__IPIPE_SYSCALL_R;
-		if (__ipipe_current_domain != ipd)
-			/* Account for domain migration. */
-			this_domain = __ipipe_current_domain;
-		else
-			__ipipe_set_current_domain(this_domain);
-	}
-
-	if (this_domain == ipipe_root_domain) {
-		if (ipd != ipipe_root_domain && ret == 0) {
-			ipd = ipipe_root_domain;
-			goto next;
+		caller_domain = this_domain = __ipipe_current_domain;
+		ipd = ipipe_head_domain;
+next:
+		p = ipipe_this_cpu_context(ipd);
+		if (likely(p->coflags & __IPIPE_SYSCALL_E)) {
+			__ipipe_set_current_context(p);
+			p->coflags |= __IPIPE_SYSCALL_R;
+			hard_local_irq_restore(flags);
+			ret = ipipe_syscall_hook(caller_domain, regs);
+			flags = hard_local_irq_save();
+			p->coflags &= ~__IPIPE_SYSCALL_R;
+			if (__ipipe_current_domain != ipd)
+				/* Account for domain migration. */
+				this_domain = __ipipe_current_domain;
+			else
+				__ipipe_set_current_domain(this_domain);
 		}
-		/*
-		 * Careful: we may have migrated from head->root, so p
-		 * would be ipipe_this_cpu_context(head).
-		 */
-		p = ipipe_this_cpu_root_context();
-		if (__ipipe_ipending_p(p))
-			__ipipe_sync_stage();
- 	} else if (ipipe_test_thread_flag(TIP_MAYDAY))
-		__ipipe_call_mayday(regs);
 
-	hard_local_irq_restore(flags);
+		if (this_domain == ipipe_root_domain) {
+			if (ipd != ipipe_root_domain && ret == 0) {
+				ipd = ipipe_root_domain;
+				goto next;
+			}
+			/*
+			* Careful: we may have migrated from head->root, so p
+			* would be ipipe_this_cpu_context(head).
+			*/
+			p = ipipe_this_cpu_root_context();
+			if (__ipipe_ipending_p(p))
+				__ipipe_sync_stage();
+		} else if (ipipe_test_thread_flag(TIP_MAYDAY))
+			__ipipe_call_mayday(regs);
 
-	return ret;
+		hard_local_irq_restore(flags);
+
+		return ret;
+	}
 }
 
 int __weak ipipe_trap_hook(struct ipipe_trap_data *data)
@@ -1041,34 +1043,36 @@ int __weak ipipe_trap_hook(struct ipipe_trap_data *data)
 
 int __ipipe_notify_trap(int exception, struct pt_regs *regs)
 {
-	struct ipipe_percpu_domain_data *p;
-	struct ipipe_trap_data data;
-	unsigned long flags;
-	int ret = 0;
+	{ if (ipipe_notifier_enabled_p(current)) return ((int (*)(unsigned long, void *))ipipe_root_domain->legacy.handlers[exception])(exception, regs); } {
+		struct ipipe_percpu_domain_data *p;
+		struct ipipe_trap_data data;
+		unsigned long flags;
+		int ret = 0;
 
-	flags = hard_local_irq_save();
+		flags = hard_local_irq_save();
 
-	/*
-	 * We send a notification about all traps raised over a
-	 * registered head domain only.
-	 */
-	if (__ipipe_root_p)
-		goto out;
+		/*
+		* We send a notification about all traps raised over a
+		* registered head domain only.
+		*/
+		if (__ipipe_root_p)
+			goto out;
 
-	p = ipipe_this_cpu_head_context();
-	if (likely(p->coflags & __IPIPE_TRAP_E)) {
-		p->coflags |= __IPIPE_TRAP_R;
-		hard_local_irq_restore(flags);
-		data.exception = exception;
-		data.regs = regs;
-		ret = ipipe_trap_hook(&data);
-		flags = hard_local_irq_save();
-		p->coflags &= ~__IPIPE_TRAP_R;
-	}
+		p = ipipe_this_cpu_head_context();
+		if (likely(p->coflags & __IPIPE_TRAP_E)) {
+			p->coflags |= __IPIPE_TRAP_R;
+			hard_local_irq_restore(flags);
+			data.exception = exception;
+			data.regs = regs;
+			ret = ipipe_trap_hook(&data);
+			flags = hard_local_irq_save();
+			p->coflags &= ~__IPIPE_TRAP_R;
+		}
 out:
-	hard_local_irq_restore(flags);
+		hard_local_irq_restore(flags);
 
-	return ret;
+		return ret;
+	}
 }
 
 int __weak ipipe_kevent_hook(int kevent, void *data)
@@ -1078,26 +1082,28 @@ int __weak ipipe_kevent_hook(int kevent, void *data)
 
 int __ipipe_notify_kevent(int kevent, void *data)
 {
-	struct ipipe_percpu_domain_data *p;
-	unsigned long flags;
-	int ret = 0;
+	{ return ((int (*)(unsigned long, void *))ipipe_root_domain->legacy.handlers[IPIPE_FIRST_EVENT + kevent])(IPIPE_FIRST_EVENT + kevent, data); } {
+		struct ipipe_percpu_domain_data *p;
+		unsigned long flags;
+		int ret = 0;
 
-	ipipe_root_only();
-
-	flags = hard_local_irq_save();
+		ipipe_root_only();
 
-	p = ipipe_this_cpu_root_context();
-	if (likely(p->coflags & __IPIPE_KEVENT_E)) {
-		p->coflags |= __IPIPE_KEVENT_R;
-		hard_local_irq_restore(flags);
-		ret = ipipe_kevent_hook(kevent, data);
 		flags = hard_local_irq_save();
-		p->coflags &= ~__IPIPE_KEVENT_R;
-	}
 
-	hard_local_irq_restore(flags);
+		p = ipipe_this_cpu_root_context();
+		if (likely(p->coflags & __IPIPE_KEVENT_E)) {
+			p->coflags |= __IPIPE_KEVENT_R;
+			hard_local_irq_restore(flags);
+			ret = ipipe_kevent_hook(kevent, data);
+			flags = hard_local_irq_save();
+			p->coflags &= ~__IPIPE_KEVENT_R;
+		}
 
-	return ret;
+		hard_local_irq_restore(flags);
+
+		return ret;
+	}
 }
 
 void __weak ipipe_migration_hook(struct task_struct *p)
@@ -1196,48 +1202,8 @@ void __ipipe_notify_vm_preemption(void)
 EXPORT_SYMBOL_GPL(__ipipe_notify_vm_preemption);
 #endif /* CONFIG_IPIPE_HAVE_VM_NOTIFIER */
 
-static void dispatch_irq_head(unsigned int irq) /* hw interrupts off */
-{
-	struct ipipe_percpu_domain_data *p = ipipe_this_cpu_head_context(), *old;
-	struct ipipe_domain *head = p->domain;
-
-	if (unlikely(test_bit(IPIPE_STALL_FLAG, &p->status))) {
-		__ipipe_set_irq_pending(head, irq);
-		return;
-	}
-
-	/* Switch to the head domain if not current. */
-	old = __ipipe_current_context;
-	if (old != p)
-		__ipipe_set_current_context(p);
-
-	p->irqall[irq]++;
-	__set_bit(IPIPE_STALL_FLAG, &p->status);
-	barrier();
-	head->irqs[irq].handler(irq, head->irqs[irq].cookie);
-	__ipipe_run_irqtail(irq);
-	hard_local_irq_disable();
-	p = ipipe_this_cpu_head_context();
-	__clear_bit(IPIPE_STALL_FLAG, &p->status);
-
-	/* Are we still running in the head domain? */
-	if (likely(__ipipe_current_context == p)) {
-		/* Did we enter this code over the head domain? */
-		if (old->domain == head) {
-			/* Yes, do immediate synchronization. */
-			if (__ipipe_ipending_p(p))
-				__ipipe_sync_stage();
-			return;
-		}
-		__ipipe_set_current_context(ipipe_this_cpu_root_context());
-	}
-
-	/*
-	 * We must be running over the root domain, synchronize
-	 * the pipeline for high priority IRQs (slow path).
-	 */
-	__ipipe_do_sync_pipeline(head);
-}
+void (*rtai_irq_handler)(int) = NULL;
+EXPORT_SYMBOL(rtai_irq_handler);
 
 void __ipipe_dispatch_irq(unsigned int irq, int flags) /* hw interrupts off */
 {
@@ -1338,7 +1304,7 @@ void __ipipe_dispatch_irq(unsigned int irq, int flags) /* hw interrupts off */
 		if (unlikely(flags & IPIPE_IRQF_NOSYNC))
 			__ipipe_set_irq_pending(ipd, irq);
 		else
-			dispatch_irq_head(irq);
+			rtai_irq_handler(irq);
 		return;
 	}
 
@@ -1499,6 +1465,8 @@ respin:
 	__clear_bit(IPIPE_STALL_FLAG, &p->status);
 }
 
+EXPORT_SYMBOL(__ipipe_do_sync_stage);
+
 void __ipipe_call_mayday(struct pt_regs *regs)
 {
 	unsigned long flags;
diff --git a/kernel/ipipe/timer.c b/kernel/ipipe/timer.c
index e71f26b..a32b41e6 100644
--- a/kernel/ipipe/timer.c
+++ b/kernel/ipipe/timer.c
@@ -295,6 +295,8 @@ err_remove_all:
 	return -ENODEV;
 }
 
+EXPORT_SYMBOL(ipipe_select_timers);
+
 static void ipipe_timer_release_sync(void)
 {
 	struct ipipe_timer *timer = __ipipe_raw_cpu_read(percpu_timer);
@@ -319,17 +321,7 @@ void ipipe_timers_release(void)
 	}
 }
 
-static void __ipipe_ack_hrtimer_irq(unsigned int irq, struct irq_desc *desc)
-{
-	struct ipipe_timer *timer = __ipipe_raw_cpu_read(percpu_timer);
-
-	if (desc)
-		desc->ipipe_ack(irq, desc);
-	if (timer->ack)
-		timer->ack();
-	if (desc)
-		desc->ipipe_end(irq, desc);
-}
+EXPORT_SYMBOL(ipipe_timers_release);
 
 int ipipe_timer_start(void (*tick_handler)(void),
 		      void (*emumode)(enum clock_event_mode mode,
@@ -349,9 +341,7 @@ int ipipe_timer_start(void (*tick_handler)(void),
 
 	flags = ipipe_critical_enter(NULL);
 
-	ret = ipipe_request_irq(ipipe_head_domain, timer->irq,
-				(ipipe_irq_handler_t)tick_handler,
-				NULL, __ipipe_ack_hrtimer_irq);
+	ret = 0;
 	if (ret < 0 && ret != -EBUSY) {
 		ipipe_critical_exit(flags);
 		return ret;
@@ -386,6 +376,8 @@ int ipipe_timer_start(void (*tick_handler)(void),
 	return ret;
 }
 
+EXPORT_SYMBOL(ipipe_timer_start);
+
 void ipipe_timer_stop(unsigned cpu)
 {
 	unsigned long __maybe_unused flags;
@@ -422,6 +414,8 @@ void ipipe_timer_stop(unsigned cpu)
 	ipipe_free_irq(ipipe_head_domain, timer->irq);
 }
 
+EXPORT_SYMBOL(ipipe_timer_stop);
+
 void ipipe_timer_set(unsigned long cdelay)
 {
 	unsigned long tdelay;
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 987f2d6..9344aec 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -2359,63 +2359,73 @@ asmlinkage __visible void schedule_tail(struct task_struct *prev)
  * context_switch - switch to the new MM and the new
  * thread's register state.
  */
-static inline int
+static int
 context_switch(struct rq *rq, struct task_struct *prev,
 	       struct task_struct *next)
 {
 	struct mm_struct *mm, *oldmm;
 
-	prepare_task_switch(rq, prev, next);
-
 	mm = next->mm;
 	oldmm = prev->active_mm;
-	/*
-	 * For paravirt, this is coupled with an exit in switch_to to
-	 * combine the page table reload and the switch backend into
-	 * one hypercall.
-	 */
-	arch_start_context_switch(prev);
 
-	if (!mm) {
-		next->active_mm = oldmm;
-		atomic_inc(&oldmm->mm_count);
-		enter_lazy_tlb(oldmm, next);
-	} else
-		switch_mm(oldmm, mm, next);
+	if (!rq) {
+		__switch_mm(oldmm, next->active_mm, next);
+		if (!mm) enter_lazy_tlb(oldmm, next);
+	} else {
+		prepare_task_switch(rq, prev, next);
 
-	if (!prev->mm) {
-		prev->active_mm = NULL;
-		rq->prev_mm = oldmm;
-	}
-	/*
-	 * Since the runqueue lock will be released by the next
-	 * task (which is an invalid locking op but in the case
-	 * of the scheduler it's an obvious special-case), so we
-	 * do an early lockdep release here:
-	 */
+		/*
+		* For paravirt, this is coupled with an exit in switch_to to
+		* combine the page table reload and the switch backend into
+		* one hypercall.
+		*/
+		arch_start_context_switch(prev);
+
+		if (!mm) {
+			next->active_mm = oldmm;
+			atomic_inc(&oldmm->mm_count);
+			enter_lazy_tlb(oldmm, next);
+		} else
+			switch_mm(oldmm, mm, next);
+		if (!prev->mm) {
+			prev->active_mm = NULL;
+			rq->prev_mm = oldmm;
+		}
+		/*
+		* Since the runqueue lock will be released by the next
+		* task (which is an invalid locking op but in the case
+		* of the scheduler it's an obvious special-case), so we
+		* do an early lockdep release here:
+		*/
 #ifndef __ARCH_WANT_UNLOCKED_CTXSW
-	spin_release(&rq->lock.dep_map, 1, _THIS_IP_);
+		spin_release(&rq->lock.dep_map, 1, _THIS_IP_);
 #endif
-
+	}
+#ifdef CONFIG_IPIPE
+	next->ptd[IPIPE_ROOT_NPTDKEYS - 1] = prev;
+#endif /* CONFIG_IPIPE */
 	context_tracking_task_switch(prev, next);
 	/* Here we just switch the register state and the stack. */
 	switch_to(prev, next, prev);
 
 	barrier();
 
-	if (unlikely(__ipipe_switch_tail()))
-		return 1;
-
-	/*
-	 * this_rq must be evaluated again because prev may have moved
-	 * CPUs since it called schedule(), thus the 'rq' on its stack
-	 * frame will be invalid.
-	 */
-	finish_task_switch(this_rq(), prev);
+	if (unlikely(rq)) {
+		if (unlikely(__ipipe_switch_tail()))
+			return 1; __ipipe_notify_kevent(-2, NULL);
 
+		/*
+		* this_rq must be evaluated again because prev may have moved
+		* CPUs since it called schedule(), thus the 'rq' on its stack
+		* frame will be invalid.
+		*/
+		finish_task_switch(this_rq(), prev);
+	}
 	return 0;
 }
 
+EXPORT_SYMBOL(context_switch);
+
 /*
  * nr_running and nr_context_switches:
  *
@@ -5770,6 +5780,7 @@ cpu_attach_domain(struct sched_domain *sd, struct root_domain *rd, int cpu)
 
 /* cpus with isolated domains */
 static cpumask_var_t cpu_isolated_map;
+EXPORT_SYMBOL(cpu_isolated_map);
 
 /* Setup the mask of cpus configured for isolated domains */
 static int __init isolated_cpu_setup(char *str)
-- 
1.8.3.1

